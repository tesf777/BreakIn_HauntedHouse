![[1.png]]
![[Pasted image 20221011094742.png]]
![[2.png]]
## 一.前提
AI的任务是设计Agent程序，实现的是把感知信息映射到行动的Agent函数。
## 二.任务环境

### 2.1 PEAS描述
- Performance性能：**到达出口的成功率，到达出口的行动步数**
- Environment环境：**6X6方格内，存在两个随机行动的幽灵，存在围墙**
- Actuators执行器：**在自身四邻域内移动，也可以不行动，不能穿墙**
- Sensors传感器：**通过曼哈顿距离感知幽灵，感知可以叠加**
### 2.2 性质
2.2.1 完全可观察的vs部分可观察的

    如果Agent的传感器在每个时间节点上都能获取环境的完整状态，这个任务环境就是完全可观察的。否则，则是部分可观察的。
    如果传感器能够检测所有与行动决策相关的信息，该任务环境就是有效完全可观察的。
    如果Agent根本没有传感器，环境则是无法观察的。

2.2.2 单Agent vs 多Agent

区分两者的关键在于Agent B 行为的性能度量最大化是否需要依赖于Agent A的行为。

    竞争性的多Agent环境
    Agent B想要最大化自己的性能度量，就需要最小化Agent A的性能度量。

    合作性的多Agent环境
    Agent B想要最大化自己的性能度量，就需要最大化Agent A的性能度量。

    部分合作部分竞争的多Agent环境
    上述两种情况都会发生。

2.2.3 确定的 vs 随机的

如果环境的下一个状态完全取决于当前状态和Agent执行的动作，则该环境是确定的；否则，是随机的。

    不确定与随机的区别
        环境不确定是指 环境不是完全可观察的或不确定的，行动后果可能有多种，但与概率无关。
        环境随机是指后果是不确定的并且可以用概率来量化。

2.2.4 片段式 vs 延续式

片段式是指 当前决策不会影响到未来的决策。
延续式是指 当前决策会影响到所有未来的决策。
2.2.5 静态 vs 动态

如果环境在Agent计算的时候会变化，该环境是动态的，否则是静态的。
如果环境本身不随时间变化而变化，但Agent的性能评价随时间变化，则环境是半动态的。
2.2.6 离线 vs 连续

离线和连续的使用场景有：

    环境的状态
    时间的处理方式
    Agent的感知信息和行动

2.2.7 已知 vs 未知

Agent的知识状态。
如果环境是未知的，Agent需要学习环境是如何工作的，以便做出好的决策。

问题A：部分可观察，环境随机，延续式，动态，离散的，已知环境
问题B：部分可观察，环境随机，延续式，动态，离散的，已知环境
问题C：完全可观察，环境确定，延续式，动态，离散的，已知环境
## 3 Agent的结构
3.1 基于模型的反射Agent

    定义

        世界模型：
            知识一：世界是如何独立于Agent而发展的信息
            知识二：Agent自身的行动如何影响世界

        使用世界模型的Agent称为基于模型的Agent
        通过将当前的感知信息与过去的内部状态结合来更新当前状态
        针对部分可观测环境

    缺点
    部分可观察环境中的Agent不能精准确定当前状态

![[Pasted image 20221011102630.png]]
3.2 基于效用的Agent

    定义
    理性的基于效用的Agent 选择 期望效用最大化的行动，Agent在给定每个结果的概率和效用下，期望得到的平均效用。
    方法
    Step 1：使用关于世界的模型，以及对各个世界状态的偏好程度进行度量的效用函数。
    Step 2：选择可以取得最佳期望效用的行动。
    Step 3：通过结果的概率来确定权值，最佳期望效用是通过计算所有可能结果状态的加权平均值得到的。
    适用情况
    1、当多个目标互相冲突时，只有其中一些目标可以达到时，效用函数可以在它们之间适当的折中。
    2、当Agent有几个目标，但没有一个有把握达到时，效用函数可以在它们之间适当的折中。

![[Pasted image 20221011102756.png]]
















